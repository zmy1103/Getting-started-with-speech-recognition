## 食物声音识别（一.Baseline学习）

![](https://pic4.zhimg.com/80/dd6786b56762f1257060e2956bf1c825_1440w.jpg?source=1940ef5c)

- #### 基于CNN的Baseline:

  1. 用到的基本库解析：

     ```python
     from sklearn.model_selection import train_test_split #分割测试集和训练集
     from sklearn.metrics import classification_report #显示主要分类指标的报告．显示每个类的精确度，召回率，F1值等信息
     from sklearn.model_selection import GridSearchCV #自动调参
     from sklearn.preprocessing import MinMaxScaler #数据归一化，归一到[0,1]
     import os
     import glob #匹配所有的符合条件的文件，并将其以list的形式返回
     #“*”表示匹配任意字符串，“?”匹配任意单个字符，[0-9]与[a-z]表示匹配0-9的单个数字与a-z的单个字符。
     #遍历顺序默认按名字，但可以进行排序
     #sorted(glob.glob('*.png'), key=os.path.getsize) 按文件大小
     #key=os.path.getmtime按修改时间
     from tqdm import tqdm #在Python长循环中添加一个进度提示信息
     ```

  2. 深度学习框架

     ```python
     from tensorflow.keras.models import Sequential
     from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D, Dropout
     from tensorflow.keras.utils import to_categorical #将类别向量转换为二进制（只有0和1）的矩阵类型表示
     
     from sklearn.ensemble import RandomForestClassifier
     from sklearn.svm import SVC
     ```

  3. 语音处理基础知识

     ​	在语音识别和话者识别方面，最常用到的语音特征就是梅尔倒谱系数（Mel-scaleFrequency Cepstral Coefficients，简称MFCC）。MFCC提取过程包括预处理、快速傅里叶变换、Mei滤波器组、对数运算、离散余弦变换、动态特征提取等步骤。

     ​	目前项目中只要知道`np.mean(librosa.feature.melspectrogram(y=X,sr=sample_rate).T,axis=0)`代表提取的语音特征就可以了。

     参考：https://zhuanlan.zhihu.com/p/181718235

     ![](https://pic3.zhimg.com/80/v2-ce1e7a7949c3ad0ad23e109779b916be_1440w.jpg)

  4. 音视频处理库

     LibROSA是一个用于音乐和音频分析的python包。它提供了创建音乐信息检索系统所需的构建块。

     | 函数                                                         | 含义                                                         | 返回值               |
     | ------------------------------------------------------------ | ------------------------------------------------------------ | -------------------- |
     | librosa.load(path, sr=22050, mono=True, offset=0.0, duration=None) | 读取音频文件。默认采样率是*22050，如果*要保留音频的原始采样率，使用*sr = None*。 | 音频时间序列；采样率 |
     | librosa.resample(y, orig_sr, target_sr, fix=True, scale=False) | 重新采样从orig_sr到target_sr的时间序列                       | 重采样之后的音频数组 |
     | librosa.get_samplerate(path)                                 | 读取采样率                                                   | 采样率               |
     | soundfile.write(file, data, samplerate)                      | 将时间序列输出为.wav文件                                     |                      |
     | librosa.display.waveplot(y, sr=22050, x_axis='time', offset=0.0, ax=None) | 绘制波形的幅度包络线                                         |                      |

     参考：https://www.cnblogs.com/LXP-Never/p/11561355.html

  5. 代码总体思路

     1. 特征提取，建立训练集和验证集。

        建立类别标签，双向key的字典，方便索引。

        遍历所有wav文件，计算梅尔频谱，并把它作为特征

     2. 搭建CNN网络，进行训练。

     3. 对测试集进行预测保存

  6. CNN网络结构

     ![image-20210413182450606](http://ww1.sinaimg.cn/large/005IQUPRly1gpian5kx1jj32zs1mce7o.jpg)

  7. 知识积累和收获

     ```python
     1.transpose()重新调整下标的位置
     2.np.vstack
     3.验证集（validation set）—— 是模型训练过程中单独留出的样本集，它可以用于调整模型的超参数和用于对模型的能力进行初步评估。可以在样本集中分离出来：X_train,X_test, y_train, y_test = train_test_split(train_data,train_target,test_size=0.4, random_state=0, stratify=Y)
     # random_state：是随机数的种子。
     # 随机数种子：其实就是该组随机数的编号，在需要重复试验的时候，保证得到一组一样的随机数。比如你每次都填1，其他参数一样的情况下你得到的随机数组是一样的。但填0或不填，每次都会不一样。
     #stratify是为了保持split前类的分布。将stratify=X就是按照X中的比例分配,将stratify=y就是按照y中的比例分配
     比如有100个数据，80个属于A类，20个属于B类。如果train_test_split(... test_size=0.25, stratify = y_all), 那么split之后数据如下： 
     training: 75个数据，其中60个属于A类，15个属于B类。 
     testing: 25个数据，其中20个属于A类，5个属于B类。 
     4.模型的参数包括普通参数和超参数（与模型设计和训练有关的一些参数），利用bp只能训练普通参数，而无法“训练”模型的超参数，因此，我们设置了验证集，通过验证集的效果进行反馈，根据效果看是否需要终止当前的模型训练，更改超参之后再训练，最终得到最优的模型！
     5.损失函数：交叉熵
     其他的损失函数小结：https://zhuanlan.zhihu.com/p/77686118
     6.评价指标：准确率
     评价指标小结：https://zhaokv.com/machine_learning/2016/03/ml-metric.html
     ```

- #### 基于LSTM的Baseline：